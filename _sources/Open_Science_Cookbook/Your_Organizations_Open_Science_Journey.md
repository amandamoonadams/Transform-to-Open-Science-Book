# Guide for Your Organization's Open Science Journey

In recent decades, the technological barriers to participating in open science have decreased, but barriers due to bureaucracy, funding practices, and research culture remain. Fully engaging with open science is thus dependent on individuals entering into dialogue with their universities, agencies, and companies to identify changes which will support the movement towards open science. 

Building on the work of others over the past several years, TOPS invites everyone to examine the research practices which they support and encourage, and move towards a more open framework. Join us in our journey, and encourage the pursuit of open scientific progress!  

Suggestions for how to get started examining where your organization is on its open science journey are in **Section 1: Engage with Open Science**.

Suggestions for how to prompt and lead listening sessions, workshops, and open meetings centered on setting new open science policies are in **Section 2: Holding Discussions on Open Science**. 

Interested in learning how you can collaborate with TOPS? Jump ahead to **Section 3: Working With TOPS**.

## Section 1: Engage with Open Science

Is your entire organization ready to embark on an open science journey?

### Step 1: Decide on the definition of open science that your organization will use moving forward.

TOPS has adopted the definition of open science put forth by the Office of Science and Technology Policy:

*"Open Science is the principle and practice of making research products and processes available to all, while respecting diverse cultures, maintaining security and privacy, and fostering collaborations, reproducibility, and equity."*

In our resources, TOPS breaks this definition down further, discussing it in terms of data, software, results, tools and an open science ethos. Having such a definition can serve as a north star for conversation, and we invite all organizations to clearly communicate the definition of open science that it is using to staff and stakeholders. If your organization is brand-new to open science, hosting a public discussion to set this definition is a good way to get folks invested in the work. 

### Step 2: Collect and make findable open science policies and practices that are already in place.

Consider your organization’s current practices. For example, do researchers use open-source code, are pre-prints encouraged, or are studies pre-registered? What policies are in place regarding sharing instrument data, user surveys, and detailed methodology? How about instructions regarding licenses and copyrights? Demonstrate where your organization is on its open science journey by making it easier for everyone to find and access the relevant resources and policies.

Start by examining official policies at your organization. Areas to begin include mentions of pre-registration, pre-prints, open access, open-source software and/or code, data sharing or data management, open access publication or publication archiving, licenses, copyrights, meta-data, persistent digital identifiers, and digital object identifiers. Additionally, the official policy around collaborations with other organizations or government entities could either help or hinder open science. 

Collect the relevant policies in one location and, if possible, write a directory or table of contents to passages in these policies that specifically align with open science principles. 

Next, examine any “unofficial” policies. What are the research norms at your organization? Can these be updated to be more inclusive, equitable, accessible, or transparent? Address such unofficial procedures, as appropriate, in a statement that accompanies the new directory.  

### Step 3: Provide examples of open science success stories in your organization.

When highlighting how your organization is already engaging in open science, you provide those just beginning their open science journey with a model to follow, as well as a place to ask questions. Therefore, to ensure the success of these researchers, we recommend starting a repository of open science success stories for your organization. 

<!--
NASA has some examples [here](https://science.nasa.gov/open-science/transform-to-open-science/stories). A great example also came out of an ongoing collaboration by Arizona State University and the Open Research Funders Group, in conjunction with the National Academies of Sciences, Engineering, and Medicine. It is called [Project Open](https://projectopen.io/), and focuses on collecting information on how open science broadly benefits researchers.
-->

A great example came out of an ongoing collaboration by Arizona State University and the Open Research Funders Group, in conjunction with the National Academies of Sciences, Engineering, and Medicine. It is called [Project Open](https://projectopen.io/), and focuses on collecting information on how open science broadly benefits researchers.

For your organization, we suggest highlighting specific ways that existing policies allow for open science practice.

### Step 4: Discuss what is missing from current policy, involving all stakeholders in those conversations.

The US federal government has [taken steps](https://github.com/nasa/Transform-to-Open-Science/tree/main/docs/Area4_Moving_To_Openness/Open_Timeline.md) towards mandating the open sharing of publications and data produced by US government funding. Additionally, there are several [global policies](https://github.com/nasa/Transform-to-Open-Science/tree/main/docs/Area4_Moving_To_Openness/global_policies.md) supporting the move to open science. 

Once an organization knows which policies currently support open science, they can more easily compare those policies to government guidance and field best practices, and move to address any gaps. Additionally, organizations should examine the practices of the most popular journals or publication venues for the researchers of their organization. Any conflicting guidance should be noted, and addressed in the frameworks created for your organization. In some cases, organizations may need to provide greater detail or guidance than is currently available, to account for the needs and practices of a particular community or scientific field.

Be sure to involve your organization’s researchers and scientists in the process of creating any new policies; hold listening sessions, town halls, or office hours in which those most affected by these policies can provide feedback, and perhaps reveal barriers to adoption. 

**In need of discussion questions? Jump ahead to Section 2!**

### Step 5: Consider crafting an Open Science Action Plan for your organization.

There are several cultural and institutional barriers which have prevented the adoption of open science in the past. Some of these are born from the academic incentive system, while others are reinforced by the desire to monetize research. Organizations with clear policies that support the sharing of resources, data, code, and results such that they can be utilized by others encourage the adoption of open science. 

Open Science Action Plans (OSAPs) are comprehensive strategy documents designed to advance adoption of open science practices. They should document: 

(1) what an organization is doing well, 

(2) where there are areas for improvement, and 

(3) detail specific actions that the organization will take to address the areas of improvement. 

Some organizations may be hesitant to publicly commit themselves to OSAPs; it can be difficult to acknowledge areas where there is still work to be done. Part of adopting open science is becoming more comfortable sharing our failures -- as individuals and organizations -- as well as our successes. Honestly sharing a current status, publishing lessons learned, and announcing ideas for moving forward, are exactly the open science practices that TOPS and the greater open science community are advocating scientists adopt. 

One possible starting point for an OSAP  is to use the [UNESCO Recommendation on Open Science](https://en.unesco.org/science-sustainable-future/open-science/recommendation) which sets out 7 areas of action to advance open science. Within that report, there are specific examples for different types of organizations. 

These areas of action are:
* Promoting   a   common   understanding   of   open   science,   associated   benefits   and   challenges, as well as diverse paths to Open Science
* Developing an enabling policy environment for open science
* Investing in open science infrastructures and services
* Investing in human resources, education, digital literacy and capacity building for open science
* Fostering a culture of open science and aligning incentives for open science
* Promoting innovative approaches for open science at different stages of the scientific process
* Promoting  international  and  multistakeholder  cooperation  in  the  context  of  open  science, and in view of reducing digital and knowledge gaps

Not sure where these investments need to be made? Section 2 of this guide has some questions to consider. 

Once an open science plan is developed, ideally, it is shared publicly alongside clearly defined metrics. Here are some example activities that could be a part of that plan, sorted by type of organization:
* **Science Associations:** 
     * Designate one day of annual meetings to focus on open science education. This could include workshops, tutorials, and showcases, and should aim to represent all the aspects of participating in and contributing to open science. 
     * Consider creating open science awards and include open science activities in criteria for existing honors where possible, and allow award nominations to include teams. 
* **Federal Agencies:** 
     * Designate one day each month to advance open science practices. 
     * Work to incentivise open science activities, and provide training opportunities for learning open science tools. 
     * Update agency evaluation, promotion criteria, and guidelines to better enable participation in open science.
* **Academic Institutions:** 
     * Update evaluation and promotion criteria to include recognition of open science activities. 
     * Incorporate open science best practices into graduate and undergraduate curricula. 
     * Update intellectual property guidelines to align with open initiatives. 
     * Develop open science cohorts within departments to support the move towards openness.

## Section 2: Holding Discussions on Open Science

It can be a bit difficult to know where to begin when you are updating existing procedures to better account for an open science ethos. Using the sources we cite below, we put together a list of "food for thought" questions to discuss as an organization.

### Discussing Research and Data Products
* Consider the data held in trust by your organization: 
    * Where can researchers find your data? 
    * What is your process/method for releasing data? 
    * Can portions of it be anonymized such that it can be shared? 
    * Is the process for requesting access to your data clear? 
    * How fast is the data request process and how often do you go through the process of releasing data? 
    * Does your organization have practices around pre-registration and archiving? 
    * Are these practices well-known and front-of-mind?
    * An example of a tool that allows searching of open databases is [Registry of Research Data Repositories](https://www.re3data.org/)
* If your organization produces research, consider if:
    * Research plans may be made available, prior to the start of the project, 
    * That reproducibility/replicability studies be valued and encouraged, and
    * That null/negative results be published. 
    * Some examples of making research plans available can be found at [AsPredicted](https://aspredicted.org/), [Open Science Framework](https://osf.io/), and [Registered Reports](https://cos.io/rr/) 
    * Examples of journals that publish negative/nul results include Positively Negative (PLOS One), The Missing Pieces: A Collection of Negative; Null and Inconclusive Results (PLOS One), The All Results Journals, ACS Omega (ACS Publications), F1000Research, PeerJ, Journal of Negative Results in Biomedicine, Journal of Negative Results: Ecology and Evolutionary Biology, Journal of Articles in Support of the Null Hypothesis, Journal of Pharmaceutical Negative results and Nature Scientific Reports
    * Examples of journals that publish reproducibility/replicability studies are PLOS ONE, and [Royal Society](https://royalsocietypublishing.org/rsos/replication-studies)
* For research produced *for* your organization: 
    * Do you require that the list of materials, study methods, and computational environment be included in the final results which are shared publicly? 
* For data or research produced *by* your organization: 
    * In what languages is the research available? Are bilingual researchers encouraged to apply for funding or publish in multiple languages? 
    * What services are available for translation of research produced by your organization?
* Does your organization develop software? 
    * Can this code be made publicly available via GitHub, BitBucket, Zenodo, or other mechanisms? 
    * Consider how much of the work of your group ought to be kept “closed” and how much can be made available to advance the work of others, particularly young professionals and early career researchers.
  
### Discussing Metrics and Incentives

According to [Suominen *et al*](https://doi.org/10.1016/j.techfore.2021.120882) motivating forces for researchers and scientists are “curiosity, good practice, high-quality science, and making a difference” while de-motivating factors include “collaboration problems, competition, and lack of feedback and recognition for management.” The article goes on to conclude that motivating factors tend to be intrinsic, while the de-motivating factors were often environmental and specific to the organization where the researcher worked. With this in mind, consider the following: 

* If your organization conducts research, consider the metrics used when evaluating professors, researchers, lab assistants etc. for promotions and opportunities. 
    * What do these metrics *actually* measure? 
     * Do these metrics account for historic bias, institutional bias, or other inclusivity or accessibility considerations? 
     * Do these metrics account for time and effort spent on research that ultimately produced null and/or negative results? 
    * Explore using alternative metrics that value transparency, reproducibility, replicability, and access. In particular, consider metrics which reflect readability and accessibility of software, code, and data, to encourage and reward researchers who spend time and resources on data science principles 
    * To learn more about this topic, explore work by [Fire and Guestrin](https://academic.oup.com/gigascience/article/8/6/giz053/5506490); [Beall](https://pubs.acs.org/doi/10.1021/acs.jpclett.5b00910); and [Carpenter, Cone and Sarli](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4987709/) in analyzing the impact of traditional metrics for journal publications and research merit 
* Does your institution incentive and reward researchers in which the primary output of the research is code or software development? Consider highlighting work which results in “good code” and valuing publication in “code first” journals equally to those published in other, more traditional journals. 
  * The Journal of Open Source Software and Journal of Open Research Software are two examples of journals which allow for the "traditional" publication of code
  * Consider your organization’s current incentive and recognition structure: is it recognizing motivating factors in those it hires, promotes and praises?
  *  Consider the overall structure for management of research: are any of the de-motivating factors present? Such factors can be minimized by providing researchers with opportunities (both informal and formal) to network, ask for and receive constructive feedback and share their work with those in other fields in addition to their own. 
* It is the suggestion of the [National Academy of Sciences](https://doi.org/10.17226/26308) that organizations which are committed to moving towards and advocating for open science should consider the language which is used not only in grants and calls for proposals, but also in prompts for cover letters and resumes, to ensure that the ethos of open science is consistently present. 
    * For example, are applicants for a job asked about their commitment to open science in their recent work? Are post-docs encouraged to make their data open when conducting their work? 
    * An excellent resource on this topic can be found in the [National Academies of Sciences, Engineering, and Medicine. 2021. Developing a Toolkit for Fostering Open Science Practices: Proceedings of a Workshop. Washington, DC: The National Academies Press](https://doi.org/10.17226/26308). 

### Discussing Representation, and Mentoring and Training Practices 

* What is the “signaling language” used in grant applications, job postings, and descriptions of research conducted at your organization? 
    * Does this language imply open or closed science practice? 
    * In particular, if your organization has an employee or researcher handbook, review it for language that signals a preference for open or closed science practices. For example, stressing commercialization of ideas encourages people to *not* share their findings in case it could become valuable intellectual property or trade secrets in the future. In contrast, stressing the importance of data sharing can directly enforce an open science ethos. 
* If your organization hires post-docs, are they encouraged to engage in open science?
* If your organization provides or manages funds for research, consider how the funds are used. Beyond producing results, are funds used for…
    * Research at the intersection of science and society? 
    * Research done by underrepresented/minority researchers? 
    * Research done to directly support underrepresented/minority communities?
    * Research done by early career researchers? 
    * Research done by students (undergraduate and graduate levels)? 
    * Research done to replicate and/or reproduce other studies?
    * Research that produces code? (And do the requirements maintain that this code must be Findable, Accessible, Interoperable, Reusable (FAIR)?)
    * Research that produces data? (And do the requirements maintain that the data must be properly assigned metadata and be made available alongside the results OR prior to the results being published OR in a publicly accessible database so that others may use it?)  
    * Research in which the primary result is code or software development? 
* Does your organization provide training in open science practices such as the use of open databases, or the sharing of open-source software? How about training in bias and anti-bias practices both in the distribution of funding, and in the interpretation of research results (e.g., data dredging, p-hacking and HARK-ing)? 
    * Consider adopting and adapting the [TOPS Open Science curriculum](https://github.com/nasa/Transform-to-Open-Science/tree/main/docs/Area2_Capacity_Sharing/), the work of the European Union’s [FOSTER project](https://www.fosteropenscience.eu/toolkit) or/and what has been done by the [Berkeley Initiative for Transparency in the Social Sciences (BITSS)](https://www.bitss.org/resource-library/) to your needs and incentivizing funded researchers to learn more about open science. 
* Does your organization have access to makerspaces? 
    * Who can use them? Are they open to all or only to a certain group? 
    * Consider creating a pathway for community members–particularly high schoolers, study teams organized via libraries, Girl and Boy Scouts, YMCAs, and other organized youth groups–to be able to apply to use these spaces.
* Does your organization have a structured mentoring program for early career researchers? If not, consider creating one. If one already exists, consider incorporating themes of open science into the program.

### Some Additional Discussion Prompts for Academic Institutions 

These discussion prompts are heavily associated with academic coursework. In addition to those listed here, the [Open-Access recommendations by MIT](https://mitl.pubpub.org/pub/future-of-libraries/release/1) explores this particular perspective of advancing open science practices and culture. 

* Are both undergraduate and graduate students required to learn about proper statistical analysis and inference? 
    * In particular, do your students understand common problems with statistical analysis which can lead to a lack of accessibility, replicability, and/or reproducibility of the results (e.g., p-hacking, cherry-picking)? 
    * Consider making such knowledge a central tenet of STEM courses at both the undergraduate and graduate levels. 
* For researchers who conduct “field work:” 
    * How do they work with the communities in the field they are studying? If they are being immersed into a community, culture and/or society other than their own, what are your institution's practices and suggestions for that interaction? Are they required to know the language? Study the history and context? Are they required to have partners who are of that community, culture and/or society on their research team? 
    * Consider if your institution may benefit from adjusting the metrics for success in these situations, to include time and incentive for researchers to fully engage with the people most closely affected–either directly or indirectly–by their work. 
* On a related note, are STEM majors and those pursuing graduate studies in STEM encouraged to study other languages and/or cultures? 
     * Are the requirements for a major structured in such a way that they have room in their schedules to pursue those interests? Studying other languages can be a great way to begin to understand the motivations of another community. 
     * Next time you review requirements, consider this: what do the requirements of these majors teach students about what is important? 


### Citing Our Sources
Recommendations for this page were pulled from the following publications. Thank you to the contributors, authors, and editors of these reports for sharing the power of open science with the world.
* National Academies of Sciences, Engineering, and Medicine 2018. Open Science by Design: Realizing a Vision for 21st Century Research. Washington, DC: The National Academies Press. [https://doi.org/10.17226/25116](https://doi.org/10.17226/25116).
* OA Task Force, Mit and Katharine Dunn. 2018. “Open Access at MIT and Beyond.” MIT Open Access Task Force. [https://open-access.mit.edu/sites/default/files/20180917_Provost_OpenAccessTF_WhitePaper.pdf](https://open-access.mit.edu/sites/default/files/20180917_Provost_OpenAccessTF_WhitePaper.pdf) 
* National Academies of Sciences, Engineering, and Medicine 2019. Reproducibility and Replicability in Science. Washington, DC: The National Academies Press. [https://doi.org/10.17226/25303](https://doi.org/10.17226/25303). 
* Open Tech Strategies 2019. Open Source Archetypes: A Framework For Purposeful Open Source. [https://opentechstrategies.com/archetypes](https://opentechstrategies.com/archetypes) 
* Fogel, Karl 2020. Producing Open Source Software How to Run a Successful Free Software Project. [https://producingoss.com/en/social-infrastructure.html](https://producingoss.com/en/social-infrastructure.html#forkability) 
* Hampson, Glenn et al. 2020. “Open Science Roadmap: Recommendations to UNESCO.” [https://doi.org/10.13021/osi2020.2735](https://doi.org/10.13021/osi2020.2735) 
* National Academies of Sciences, Engineering, and Medicine. 2021. Developing a Toolkit for Fostering Open Science Practices: Proceedings of a Workshop. Washington, DC: The National Academies Press. [https://doi.org/10.17226/26308](https://doi.org/10.17226/26308). 
* Suominen, Arho. Kauppinen, Henni. Hyytinen, Kirsi. 2021. ‘Gold’, ‘Ribbon’ or ‘Puzzle’: What motivates researchers to work in Research and Technology Organizations. Technological Forecasting & Social Change. [https://doi.org/10.1016/j.techfore.2021.120882](https://doi.org/10.1016/j.techfore.2021.120882) 
* UNESCO. 2021. UNESCO Recommendation on Open Science. [https://en.unesco.org/science-sustainable-future/open-science/recommendation](https://en.unesco.org/science-sustainable-future/open-science/recommendation) 
* Martin S. Hagger (2022) Developing an open science ‘mindset’, Health Psychology and Behavioral Medicine, 10:1, 1-21, DOI: 10.1080/21642850.2021.2012474. [https://doi.org/10.1080/21642850.2021.2012474](https://doi.org/10.1080/21642850.2021.2012474)  
* Miedema, Frank 2022. Open Science: The Very Idea. Utrecht, The Netherlands: Springer Nature. [https://doi.org/10.1007/978-94-024-2115-6](https://doi.org/10.1007/978-94-024-2115-6)

## Section 3: Working With TOPS

If you are ready to do an event together, reach out to us using [this contact form](https://docs.google.com/forms/d/1XcjQU9vYyXAMmJFdB6H021PFypGYWbNKvNR_em5q2UY). You can request a meeting to learn more, suggest one of the ideas we list below, or bring us whatever ideas you may have! 

### Host a Hackathon 

Hackathons have become popular in recent years as a mechanism for getting software developers, engineers, and scientists together to think up out-of-the-box solutions for interesting or difficult problems. Hackathon participants often attend workshops or presentations, create a final poster or presentation, and have an opportunity to mingle with sponsors or leaders in their fields. Consider hosting a hackathon with an open science theme! Use either NASA's or your own organization’s open data to tackle a new problem; or challenge participants to think of a new way to overcome a barrier to adopting open science! 

**How do I get started?**

There are several resources available for organizations who have not hosted hackathons in the past. At minimum, you will need a team that will act as the hackathon coordinators, and can arrange the (physical or virtual) venue, advertise the event, help participants sign-up and choose a team, find speakers, and arrange prizes. 

**Basic Requirements**

A hackathon with a Year of Open Science theme should, at minimum:
* Use open data when solving the presented challenges. 
* Post all code generated by the hackathon openly, with as unrestrictive a license as possible. 
* Collaborate on the hackathon as openly as possible, using open science frameworks such as [Open Science Framework](https://osf.io/). 

**TOPS Will Provide**

To help make this event a success, TOPS will provide organizations with the following support:
* TOPS will amplify your event on our listserv, via Twitter, and in our community forums.
* A directory of NASA’s open data and open resources to search and use them. More information available on the pages of the [Open-Source Science Initiative](https://science.nasa.gov/open-science-overview) (OSSI).
* Year of Open Science branding packet, including templates for stickers, presentation templates, Zoom backgrounds, and a guide for the use of NASA’s and TOPS’ logo and name.


### Host a Webinar on Using Your Organization’s Open Data or Open Software

Does your organization have data, software, or publications which are freely available or only require a free login to access? Are members of your organizations aware of these resources? Are people not affiliated with your organization aware of these resources? To boost the usage of the open data, open-source software, or other open scientific information of which your organization is a steward, we recommend hosting a virtual webinar or town hall on how to use that information! 

**How do I get started?**

Prior to scheduling the session, make sure to gather any instructions related to accessing, downloading, and properly citing the data or software. If possible, plan a demo related to an open science project for the webinar, and focus on best practices for using your organization’s data or software. Be sure to record and create a transcript of the session, and upload them to your organization’s website along with any access instructions. 

**Basic Requirements**

To help make this event a success, we recommend the following:
* If your organization has open data, open-source software, a preprint or open access server, and free resources for scientific analysis we recommend *not* trying to cover them all in a single webinar. Instead, consider putting on a webinar series, with each event focusing on one type of scientific information. Similarly, if your organization has multiple types of data, with special access instructions for each type, consider splitting that webinar into a multi-part series, too. 
* Ensure that the instructions for accessing your organization’s open scientific information are easy to find. Clearly display any limitations to the usage of the data or software, and include instructions for citing the data and code of others. 
* If possible, include a demo of using the data, software etc. during the webinar, and use that to highlight those in your organization already engaged in open science.
* Contact TOPS to amplify your event! We can send your event series to our listserv and amplify your Tweets.

**TOPS Will Provide**

To help make this event a success, TOPS will provide organizations with the following support:
* TOPS will amplify your event on our listserv, via Twitter, and in our community forums.
* Year of Open Science branding packet, including templates for stickers, presentation templates, Zoom backgrounds, and a guide for the use of NASA’s and TOPS’ logo and name.

### Sponsor an Award

Your organization has researchers and scientists who have been practicing open science, perhaps even without knowing they were doing so! Similarly, one of the greatest barriers to the wide-spread adoption of open science is the worry that a large amount of time will be sunk into open science activities—pre-registering studies, releasing pre-prints, documenting code or assigning meta-data—and that the work will ultimately not be recognized. To encourage individuals to adopt additional open science practices, and to reward those who have already been practicing open science, we recommend creating open science awards!    

**How do I get started?**

Although the word “awards'' conjures images of a Nobel prize or an Oscar, smaller awards that are specific to your organization are just as valuable for your community. Consider the culture of your organization and how you recognize leaders, innovators, and trail blazers. Can you create an award recognizing open science practices in action that can be presented at an all-hands? If your organization funds research, consider an award that recognizes open science practices in work conducted using your funding. If your organization has a yearly conference or workshop, consider honoring those who conduct open science at those events!   

**Basic Requirements**

We suggest the following considerations when creating an open science award: 
* The award should consider whether the person, team or project being recognized is transparent, accessible, inclusive and reproducible.
* The award must consider individuals and teams with a diversity of experience, including diversity with respect to race, gender, geography, country of citizenship, academic background, profession, and areas of expertise.
* The award should recognize at least one aspect of open science practice, this could include, but is not be limited to:
    * Use and inclusion of open-source software, code sharing practices, and open-source repositories and tools;
    * Use and inclusion of data sharing, data repositories, curation, and/or creation of open data;
    * Use and inclusion of with open-access publishing, pre-publication, peer review, academic review and/or tenure processes, replication and reproducibility in scientific results, and virtual research environments;
    * Participation or leadership in open science communities;
    * Participation or leadership in diverse communities dedicated to advancing diversity in STEM, diversity in academia; and,
    * Participation or leadership in community organizing around equity and inclusion in STEM, climate justice, or citizen science.
* Open science is reliant upon teamwork, and recognizes that the different experiences we bring to the table strengthens a project. Consider creating a team award, rather than one that implies individual merit. 

**TOPS Will Provide**

To help make this event a success, TOPS will provide organizations with the following support:
* TOPS can amplify your event and award winners on our listserv, via Twitter, and in our community forums.
* Year of Open Science branding packet, including templates for stickers, presentation templates, Zoom backgrounds, and a guide for the use of NASA’s and TOPS’ logo and name.

### A Year of Open Science
To help catalyze the move to open science and support changing cultural norms, NASA has joined several federal agencies in recognizing [**2023 as a Year Of Open Science**](https://nasa.github.io/Transform-to-Open-Science/year-of-open-science/). Ready to join us? Head to the [Year of Open Science: Goals for Organizations](/Year_of_Open_Science_Guide/participants/readme.md) guide. 
